{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas.core.internals.managers'; 'pandas.core.internals' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-82bfa943c756>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/texts.dat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mdff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/texts.dat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdff\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'saved data does not match requested texts'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    596\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mload_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             warnings.warn(\"The file '%s' has been generated with a \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1048\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36mload_global\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1336\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1339\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGLOBAL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_global\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m   1386\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_compat_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMPORT_MAPPING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compat_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMPORT_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_getattribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas.core.internals.managers'; 'pandas.core.internals' is not a package"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from joblib import load, dump\n",
    "import os\n",
    "\n",
    "\n",
    "# all texts available for analysis\n",
    "texts = \\\n",
    "[\n",
    "('picture_dorian_gray',\"https://www.gutenberg.org/files/174/174-0.txt\"),\n",
    "('pride_prejudice', \"https://www.gutenberg.org/files/1342/1342-0.txt\"),\n",
    "('uncle_toms_cabin',\"https://www.gutenberg.org/files/203/203-0.txt\"),\n",
    "('hard_times',\"https://www.gutenberg.org/files/786/786-0.txt\"),\n",
    "('house_of_mirth',\"https://www.gutenberg.org/cache/epub/284/pg284.txt\"),\n",
    "('taming_of_the_shrew',\"https://www.gutenberg.org/files/1508/1508-0.txt\"),\n",
    "('heart_of_darkness',\"https://www.gutenberg.org/files/219/219-0.txt\"),\n",
    "('moby_dick',\"https://www.gutenberg.org/files/2701/2701-0.txt\"),\n",
    "('huck_finn',\"http://www.gutenberg.org/files/76/76-0.txt\"),\n",
    "('great_expectations',\"http://www.gutenberg.org/files/1400/1400-0.txt\"),\n",
    "('tale_two_cities',\"http://www.gutenberg.org/files/98/98-0.txt\"),\n",
    "('jekyll_hyde', \"http://www.gutenberg.org/files/11/11-0.txt\"),\n",
    "('wuthering_heights',\"http://www.gutenberg.org/cache/epub/768/pg768.txt\"),\n",
    "('beowulf', 'http://www.gutenberg.org/cache/epub/16328/pg16328.txt'),\n",
    "('anthem', 'https://www.gutenberg.org/files/1250/1250-0.txt'),\n",
    "('frankenstein', 'https://www.gutenberg.org/files/84/84-0.txt'),\n",
    "('scarlet_letter', 'https://www.gutenberg.org/files/25344/25344-0.txt'),\n",
    "('awakening','https://www.gutenberg.org/files/160/160.txt'),\n",
    "('yellow_wallpaper', 'https://www.gutenberg.org/files/1952/1952-0.txt'),\n",
    "('turn_of_the_screw', 'https://www.gutenberg.org/files/209/209-0.txt'),\n",
    "('great_gatsby','http://gutenberg.net.au/ebooks02/0200041.txt'),\n",
    "('oliver_twist','https://www.gutenberg.org/files/730/730.txt'),\n",
    "('1984','http://gutenberg.net.au/ebooks01/0100021.txt'),\n",
    "('metamorphosis', 'http://www.gutenberg.org/cache/epub/5200/pg5200.txt'),\n",
    "('jane_eyre','http://www.gutenberg.org/cache/epub/1260/pg1260.txt'),\n",
    "('tom_sawyer','https://www.gutenberg.org/files/74/74-0.txt'),\n",
    "('uncle_toms_cabin', 'https://www.gutenberg.org/files/203/203-0.txt')]\n",
    "\n",
    "def clean_text(text):\n",
    "    'remove gutenberg formatting'\n",
    "    try:\n",
    "        start = text.index(\"START OF THIS PROJECT GUTENBERG EBOOK\")\n",
    "        end = text.index(\"END OF THIS PROJECT GUTENBERG EBOOK\")\n",
    "    except:\n",
    "        try:\n",
    "            start = text.index(\"START OF THE PROJECT GUTENBERG EBOOK\")\n",
    "            end = text.index(\"END OF THE PROJECT GUTENBERG EBOOK\")\n",
    "        except:\n",
    "            try:\n",
    "                start = text.index(\"Title: The Great Gatsby Author: F. Scott Fitzgerald\")\n",
    "                end = text.index(\"THE END\")\n",
    "            except:\n",
    "                start = text.index(\"Title: Nineteen eighty-four Author: George Orwell (pseudonym of Eric Blair)\")\n",
    "                end = text.index(\"THE END\")\n",
    "\n",
    "                \n",
    "            \n",
    "    clean_text = text[start:end]\n",
    "    assert len(clean_text)>.5*len(text)\n",
    "    return clean_text\n",
    "\n",
    "def get_text(url):\n",
    "    'fetch txt for a given url'\n",
    "    \n",
    "    resp = requests.get(url)\n",
    "    text = resp.text.replace('nigger', 'SLUR') # replace offensive term with placeholder\n",
    "    text = ''.join(char for char in text if ord(char) < 128)\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return clean_text(text) \n",
    "\n",
    "def run(texts):\n",
    "    \n",
    "    df=[]\n",
    "    for t in tqdm_notebook(texts):\n",
    "        df.append((t[0], get_text(t[1])))\n",
    "        \n",
    "        \n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=10000, max_df=.5, lowercase=False, )\n",
    "    dff = pd.DataFrame(tfidf.fit_transform(df[1]).todense())\n",
    "    dff['title'] = df[0]\n",
    "    \n",
    "    return dff, df, tfidf\n",
    "\n",
    "if os.path.exists('data/texts.dat'):\n",
    "    dff, df, tfidf = load('data/texts.dat')\n",
    "    assert(len(dff) == len(texts)), 'saved data does not match requested texts'\n",
    "else:\n",
    "    dff, df, tfidf = run(texts)  \n",
    "    dump([dff, df, tfidf], 'data/texts.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract KWIC and lexical dispersion plots for keywords of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from re import finditer\n",
    "from itertools import tee, islice, chain, repeat\n",
    "from pprint import pprint \n",
    "from prettytable import PrettyTable\n",
    "from IPython.display import HTML\n",
    "from ipywidgets.widgets import interact\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def kwic(text, tgtword, width=10):\n",
    "    'Find all occurrences of tgtword and show the surrounding context'\n",
    "    matches = (mo.span() for mo in finditer(r\"[A-Za-z\\'\\-]+\", text))\n",
    "    padded = chain(repeat((0,0), width), matches, repeat((-1,-1), width))\n",
    "    t1, t2, t3 = tee((padded), 3)\n",
    "    t2 = islice(t2, width, None)\n",
    "    t3 = islice(t3, 2*width, None)\n",
    "    for (start, _), (i, j), (_, stop) in zip(t1, t2, t3):\n",
    "        if text[i: j] == tgtword:\n",
    "            context = text[start: stop]\n",
    "            yield context, (i,j)\n",
    "            \n",
    "\n",
    "@interact(word='', title=dff.title, size=range(10,25))\n",
    "def get_kwic(title='awakening',word='love', size=25):\n",
    "    \n",
    "    text = str(df[df[0]==title][1].values)\n",
    "    \n",
    "    poss=[]\n",
    "    rows=[]\n",
    "    for t, pos in list(kwic(text, word, size)):       \n",
    "        cols = t.split()\n",
    "\n",
    "        left = ' '.join(cols[0:int(size)])\n",
    "        right = ' '.join(cols[int(size)+1:-1])\n",
    "        rows.append([left, word, right])\n",
    "        poss.append(pos)\n",
    "\n",
    "    x = PrettyTable()\n",
    "    x.field_names = ['left','word','right']\n",
    "    for r in rows:\n",
    "        x.add_row(r)\n",
    "        \n",
    "    if len(rows):\n",
    "        dispersion_plot =  np.zeros(len(text))\n",
    "        for p in poss:\n",
    "            dispersion_plot[p[0]:p[1]] = 1\n",
    "\n",
    "        plt.figure(figsize=(12,5))\n",
    "        \n",
    "        s = pd.Series(dispersion_plot).rolling(5000).sum()\n",
    "        s = pd.DataFrame(s).reset_index()\n",
    "        s['index'] = s['index']/(len(s)/100)\n",
    "        s.set_index('index', inplace=True)\n",
    "        ax=plt.gca()\n",
    "        s.plot(ax=ax)\n",
    "        \n",
    "        table = x.get_html_string().replace(word,'<b>'+word+'</b>')\n",
    "        \n",
    "        return HTML(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Contextual Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(word='', title=dff.title, size=range(11,27,2))\n",
    "def get_mi(title='moby_dick',word='nature', size=27):\n",
    "\n",
    "    text = str(df[df[0]==title][1].values)\n",
    "    cv_all = CountVectorizer(max_features=100000,stop_words='english', lowercase=True)\n",
    "    m_all = cv_all.fit_transform([text]).todense().flatten()\n",
    "\n",
    "    texts = []\n",
    "    for t,pos in list(kwic(text, word, size)):       \n",
    "        cols = t.split(' ')\n",
    "        left = ' '.join(cols[0:int(size)])\n",
    "        right = ' '.join(cols[int(size)+1:-1])\n",
    "        texts.append(left +' ' + right)\n",
    "\n",
    "    if len(texts):\n",
    "\n",
    "        cv = CountVectorizer(max_features=10000, stop_words='english',lowercase=True)\n",
    "        m = cv.fit_transform(texts)\n",
    "\n",
    "        hits = m.shape[0]\n",
    "\n",
    "        names = cv.get_feature_names()\n",
    "        m = m.todense().sum(axis=0)\n",
    "\n",
    "        inds = []\n",
    "        D = {}\n",
    "        for i,item in enumerate(cv_all.get_feature_names()):\n",
    "            D[item] = i\n",
    "\n",
    "\n",
    "        for n in names:\n",
    "            inds.append(D[n])\n",
    "\n",
    "        a = np.asarray(m_all[0].tolist()[0])[np.asarray(inds)]\n",
    "\n",
    "        p_a_b = [_/(hits*size*2) for _ in m]\n",
    "        p_b = hits/len(text)\n",
    "        p_a = [_/len(text) for _ in a]\n",
    "\n",
    "        scores = [(a_b/(a*p_b)) for a_b, a in zip(p_a_b, p_a)]\n",
    "        \n",
    "        print(\"Total number of occurences: {}\".format(hits))\n",
    "        result_list = [(n, s) for n,s in zip(names, scores[0].tolist()[0])]\n",
    "        result = pd.DataFrame(result_list).sort_values(1, ascending=False).head(25)\n",
    "        \n",
    "        return result \n",
    "    \n",
    "    else:\n",
    "        print('please enter term appearing in document')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get top overall words for text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "@interact(title=dff.title)\n",
    "def get_top_terms(title='moby_dick', size=range(15,50,5)):\n",
    "    scores = dff[dff.title==title].drop('title', axis=1).values\n",
    "    inds = scores.argsort()[0][-size:]\n",
    "\n",
    "    result = pd.DataFrame([(a[0],b) for a,b in \n",
    "                           zip(np.asarray(tfidf.get_feature_names()).reshape(-1,1)[inds], \n",
    "                               scores[0][inds])])\n",
    "    result.sort_values(1, ascending=False, inplace=True)\n",
    "    return result \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Get similarity between all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "c = dff.drop('title',axis=1).T.corr()\n",
    "c = c.rename(columns={i:v for i,v in dff.title.items()},index={i:v for i,v in dff.title.items()})\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(c, vmin=-.05, vmax=.05,cmap=\"RdBu_r\")\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 2000\n",
    "n_components = 15\n",
    "n_top_words = 10\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "    \n",
    "\n",
    "def split_str(seq, chunk, skip_tail=False):\n",
    "    lst = []\n",
    "    if chunk <= len(seq):\n",
    "        lst.extend([seq[:chunk]])\n",
    "        lst.extend(split_str(seq[chunk:], chunk, skip_tail))\n",
    "    elif not skip_tail and seq:\n",
    "        lst.extend([seq])\n",
    "    return lst\n",
    "\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', lowercase=True)\n",
    "\n",
    "cv.fit(df[1].values)\n",
    "tf = cv.transform(df[1].values)\n",
    "\n",
    "\n",
    "lda.fit(tf)\n",
    "\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = cv.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
