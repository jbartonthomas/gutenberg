{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomasj/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:75: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6dc64cb676d480186f68369bcdb7ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('picture_dorian_gray', 'https://www.gutenberg.org/files/174/174-0.txt')\n",
      "('pride_prejudice', 'https://www.gutenberg.org/files/1342/1342-0.txt')\n",
      "('uncle_toms_cabin', 'https://www.gutenberg.org/files/203/203-0.txt')\n",
      "('hard_times', 'https://www.gutenberg.org/files/786/786-0.txt')\n",
      "('house_of_mirth', 'https://www.gutenberg.org/cache/epub/284/pg284.txt')\n",
      "('taming_of_the_shrew', 'https://www.gutenberg.org/files/1508/1508-0.txt')\n",
      "('heart_of_darkness', 'https://www.gutenberg.org/files/219/219-0.txt')\n",
      "('moby_dick', 'https://www.gutenberg.org/files/2701/2701-0.txt')\n",
      "('huck_finn', 'http://www.gutenberg.org/files/76/76-0.txt')\n",
      "('great_expectations', 'http://www.gutenberg.org/files/1400/1400-0.txt')\n",
      "('tale_two_cities', 'http://www.gutenberg.org/files/98/98-0.txt')\n",
      "('jekyll_hyde', 'http://www.gutenberg.org/files/11/11-0.txt')\n",
      "('wuthering_heights', 'http://www.gutenberg.org/cache/epub/768/pg768.txt')\n",
      "('beowulf', 'http://www.gutenberg.org/cache/epub/16328/pg16328.txt')\n",
      "('anthem', 'https://www.gutenberg.org/files/1250/1250-0.txt')\n",
      "('frankenstein', 'https://www.gutenberg.org/files/84/84-0.txt')\n",
      "('scarlet_letter', 'https://www.gutenberg.org/files/25344/25344-0.txt')\n",
      "('awakening', 'https://www.gutenberg.org/files/160/160-0.txt')\n",
      "('yellow_wallpaper', 'https://www.gutenberg.org/files/1952/1952-0.txt')\n",
      "('turn_of_the_screw', 'https://www.gutenberg.org/files/209/209-0.txt')\n",
      "('great_gatsby', 'http://gutenberg.net.au/ebooks02/0200041.txt')\n",
      "('oliver_twist', 'https://www.gutenberg.org/files/730/730.txt')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "substring not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c0a854450ba6>\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"START OF THIS PROJECT GUTENBERG EBOOK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"END OF THIS PROJECT GUTENBERG EBOOK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: substring not found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c0a854450ba6>\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"START OF THE PROJECT GUTENBERG EBOOK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"END OF THE PROJECT GUTENBERG EBOOK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: substring not found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c0a854450ba6>\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Title: The Great Gatsby Author: F. Scott Fitzgerald\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"THE END\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: substring not found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c0a854450ba6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdff\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'saved data does not match requested texts'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/texts.dat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c0a854450ba6>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c0a854450ba6>\u001b[0m in \u001b[0;36mget_text\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c0a854450ba6>\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"THE END\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Title: Nineteen eighty-four Author: George Orwell (pseudonym of Eric Blair)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"THE END\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: substring not found"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from joblib import load, dump\n",
    "import os\n",
    "\n",
    "\n",
    "# all texts available for analysis\n",
    "texts = \\\n",
    "[\n",
    "('picture_dorian_gray',\"https://www.gutenberg.org/files/174/174-0.txt\"),\n",
    "('pride_prejudice', \"https://www.gutenberg.org/files/1342/1342-0.txt\"),\n",
    "('uncle_toms_cabin',\"https://www.gutenberg.org/files/203/203-0.txt\"),\n",
    "('hard_times',\"https://www.gutenberg.org/files/786/786-0.txt\"),\n",
    "('house_of_mirth',\"https://www.gutenberg.org/cache/epub/284/pg284.txt\"),\n",
    "('taming_of_the_shrew',\"https://www.gutenberg.org/files/1508/1508-0.txt\"),\n",
    "('heart_of_darkness',\"https://www.gutenberg.org/files/219/219-0.txt\"),\n",
    "('moby_dick',\"https://www.gutenberg.org/files/2701/2701-0.txt\"),\n",
    "('huck_finn',\"http://www.gutenberg.org/files/76/76-0.txt\"),\n",
    "('great_expectations',\"http://www.gutenberg.org/files/1400/1400-0.txt\"),\n",
    "('tale_two_cities',\"http://www.gutenberg.org/files/98/98-0.txt\"),\n",
    "('jekyll_hyde', \"http://www.gutenberg.org/files/11/11-0.txt\"),\n",
    "('wuthering_heights',\"http://www.gutenberg.org/cache/epub/768/pg768.txt\"),\n",
    "('beowulf', 'http://www.gutenberg.org/cache/epub/16328/pg16328.txt'),\n",
    "('anthem', 'https://www.gutenberg.org/files/1250/1250-0.txt'),\n",
    "('frankenstein', 'https://www.gutenberg.org/files/84/84-0.txt'),\n",
    "('scarlet_letter', 'https://www.gutenberg.org/files/25344/25344-0.txt'),\n",
    "('awakening','https://www.gutenberg.org/files/160/160-0.txt'),\n",
    "('yellow_wallpaper', 'https://www.gutenberg.org/files/1952/1952-0.txt'),\n",
    "('turn_of_the_screw', 'https://www.gutenberg.org/files/209/209-0.txt'),\n",
    "('great_gatsby','http://gutenberg.net.au/ebooks02/0200041.txt'),\n",
    "('oliver_twist','https://www.gutenberg.org/files/730/730-0.txt'),\n",
    "('1984','http://gutenberg.net.au/ebooks01/0100021.txt'),\n",
    "('metamorphosis', 'http://www.gutenberg.org/cache/epub/5200/pg5200.txt'),\n",
    "('jane_eyre','http://www.gutenberg.org/cache/epub/1260/pg1260.txt'),\n",
    "('tom_sawyer','https://www.gutenberg.org/files/74/74-0.txt'),\n",
    "('uncle_toms_cabin', 'https://www.gutenberg.org/files/203/203-0.txt')]\n",
    "\n",
    "def clean_text(text):\n",
    "    'remove gutenberg formatting'\n",
    "    try:\n",
    "        start = text.index(\"START OF THIS PROJECT GUTENBERG EBOOK\")\n",
    "        end = text.index(\"END OF THIS PROJECT GUTENBERG EBOOK\")\n",
    "    except:\n",
    "        try:\n",
    "            start = text.index(\"START OF THE PROJECT GUTENBERG EBOOK\")\n",
    "            end = text.index(\"END OF THE PROJECT GUTENBERG EBOOK\")\n",
    "        except:\n",
    "            try:\n",
    "                start = text.index(\"Title: The Great Gatsby Author: F. Scott Fitzgerald\")\n",
    "                end = text.index(\"THE END\")\n",
    "            except:\n",
    "                start = text.index(\"Title: Nineteen eighty-four Author: George Orwell (pseudonym of Eric Blair)\")\n",
    "                end = text.index(\"THE END\")\n",
    "\n",
    "                \n",
    "            \n",
    "    clean_text = text[start:end]\n",
    "    assert len(clean_text)>.5*len(text)\n",
    "    return clean_text\n",
    "\n",
    "def get_text(url):\n",
    "    'fetch txt for a given url'\n",
    "    \n",
    "    resp = requests.get(url)\n",
    "    text = resp.text.replace('nigger', 'SLUR') # replace offensive term with placeholder\n",
    "    text = ''.join(char for char in text if ord(char) < 128)\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return clean_text(text) \n",
    "\n",
    "def run(texts):\n",
    "    \n",
    "    df=[]\n",
    "    for t in tqdm_notebook(texts):\n",
    "        print(t)\n",
    "        df.append((t[0], get_text(t[1])))\n",
    "        \n",
    "        \n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=10000, max_df=.5, lowercase=False, )\n",
    "    dff = pd.DataFrame(tfidf.fit_transform(df[1]).todense())\n",
    "    dff['title'] = df[0]\n",
    "    \n",
    "    return dff, df, tfidf\n",
    "\n",
    "if os.path.exists('data/texts.dat'):\n",
    "    dff, df, tfidf = load('data/texts.dat')\n",
    "    assert(len(dff) == len(texts)), 'saved data does not match requested texts'\n",
    "else:\n",
    "    dff, df, tfidf = run(texts)  \n",
    "    dump([dff, df, tfidf], 'data/texts.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-6-c0a854450ba6>\u001b[0m(53)\u001b[0;36mclean_text\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     51 \u001b[0;31m                \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"THE END\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     52 \u001b[0;31m            \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 53 \u001b[0;31m                \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Title: Nineteen eighty-four Author: George Orwell (pseudonym of Eric Blair)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     54 \u001b[0;31m                \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"THE END\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     55 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> text\n",
      "'<!DOCTYPE html> <html class=\"client-nojs\" lang=\"en\" dir=\"ltr\"> <head> <meta charset=\"UTF-8\"/> <title>404 | Project Gutenberg</title> <link rel=\"stylesheet\" href=\"/gutenberg/style.css?v=1.1\"> <link rel=\"stylesheet\" href=\"/gutenberg/collapsible.css?1.1\"> <link rel=\"stylesheet\" href=\"/gutenberg/new_nav.css?v=1.321231\"> <link rel=\"stylesheet\" href=\"/gutenberg/pg-desktop-one.css\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> <meta name=\"keywords\" content=\"books, ebooks, free, kindle, android, iphone, ipad\"/> <meta name=\"google-site-verification\" content=\"wucOEvSnj5kP3Ts_36OfP64laakK-1mVTg-ptrGC9io\"/> <meta name=\"alexaVerifyID\" content=\"4WNaCljsE-A82vP_ih2H_UqXZvM\"/> <link rel=\"copyright\" href=\"https://www.gnu.org/copyleft/fdl.html\"/> <link rel=\"shortcut icon\" href=\"/gutenberg/favicon.ico?v=1.1\"/> <meta property=\"og:title\" content=\"Project Gutenberg\" /> <meta property=\"og:type\" content=\"website\" /> <meta property=\"og:url\" content=\"https://www.gutenberg.org/\" /> <meta property=\"og:description\" content=\"Project Gutenberg is a library of free eBooks.\" /> <meta property=\"fb:admins\" content=\"615269807\" /> <meta property=\"fb:app_id\" content=\"115319388529183\" /> <meta property=\"og:site_name\" content=\"Project Gutenberg\" /> <meta property=\"og:image\" content=\"https://www.gutenberg.org/gutenberg/pg-logo-144x144.png\" /> </head> <body> <div class=\"container\"><!-- start body --><nav> <!--<div id=\"main_logo\"> --> <a id=\"main_logo\" href=\"/\" class=\"no-hover\"> <img src=\"/gutenberg/pg-logo-129x80.png\" alt=\"Project Gutenberg\" draggable=\"false\" /> </a> <!-- </div>--> <div id=\"menu\"> <label for=\"tm\" id=\"toggle-menu\">Menu<span class=\"drop-icon\">&#9662;</span></label> <input type=\"checkbox\" id=\"tm\" /> <ul class=\"main-menu cf\"> <li> <a href=\"/about/\">About <span class=\"drop-icon\">&#9662;</span> </a> <label title=\"Toggle Drop-down\" class=\"drop-icon\" for=\"sm0\">&#9662;</label> <input type=\"checkbox\" id=\"sm0\" /> <ul class=\"sub-menu\"> <li><a href=\"/about/\">About Project Gutenberg</a></li> <li><a href=\"/policy/collection_development.html\">Collection Development</a></li> <li><a href=\"/about/contact_information.html\">Contact Us</a></li> <li><a href=\"/about/background/\">History &amp; Philosophy</a></li> <li><a href=\"/policy/permission.html\">Permissions &amp; License</a></li> <li><a href=\"/policy/privacy_policy.html\">Privacy Policy</a></li> <li><a href=\"/policy/terms_of_use.html\">Terms of Use</a></li> </ul> </li> <li> <a href=\"/ebooks/\">Search and Browse <span class=\"drop-icon\">&#9662;</span> </a> <label title=\"Toggle Drop-down\" class=\"drop-icon\" for=\"sm8\">&#9662;</label> <input type=\"checkbox\" id=\"sm8\" /> <ul class=\"sub-menu\"> <li><a href=\"/ebooks/\">Book Search</a></li> <li><a href=\"/ebooks/bookshelf/\">Bookshelves</a></li> <li><a href=\"/browse/scores/top\">Frequently Downloaded</a></li> <li><a href=\"/ebooks/offline_catalogs.html\">Offline Catalogs</a></li> </ul> </li> <li> <a href=\"/help/\">Help <span class=\"drop-icon\">&#9662;</span> </a> <label title=\"Toggle Drop-down\" class=\"drop-icon\" for=\"sm3\">&#9662;</label> <input type=\"checkbox\" id=\"sm3\" /> <ul class=\"sub-menu\"> <li><a href=\"/help/\">All help topics &rarr;</a></li> <li><a href=\"/help/copyright.html\">Copyright Procedures</a></li> <li><a href=\"/help/errata.html\">Errata, Fixes and Bug Reports</a></li> <li><a href=\"/help/file_formats.html\">File Formats</a></li> <li><a href=\"/help/faq.html\">Frequently Asked Questions</a></li> <li><a href=\"/policy/\">Policies &rarr;</a></li> <li><a href=\"/help/public_domain_ebook_submission.html\">Public Domain eBook Submission</a></li> <li><a href=\"/help/submitting_your_own_work.html\">Submitting Your Own Work</a></li> <li><a href=\"/help/mobile.html\">Tablets, Phones and eReaders</a></li> <li><a href=\"/attic/\">The Attic &rarr;</a></li> </ul> </li> <li><a href=\"/donate/\">Donate</a></li> </ul> </div> <div class=\"donate\"> <div class=\"searchbox\"> <form method=\"get\" action=\"/ebooks/search/\" accept-charset=\"utf-8\" enctype=\"multipart/form-data\" class=\"searchbox\"> <input type=\"text\" value=\"\" id=\"menu-book-search\" name=\"query\" class=\"searchInput\" title=\"Quick search\" tabindex=\"20\" size=\"20\" maxlength=\"80\" placeholder=\" Quick search\" /> <input type=\"submit\" name=\"submit_search\" value=\"Go!\" style=\"vertical-align:middle;\" /> </form> </div> <form class=\"donatelink\" action=\"https://www.paypal.com/cgi-bin/webscr\" method=\"post\" target=\"new\"> <p><a href=\"/donate/\">Donation</a></p> <input type=\"hidden\" name=\"cmd\" value=\"_s-xclick\" /> <input type=\"hidden\" name=\"hosted_button_id\" value=\"XKAL6BZL3YPSN\" /> <input class=\"donbtn\" type=\"image\" src=\"/pics/en_US.gif\" name=\"submit\" alt=\"Donate via PayPal\" /> </form> </div> </nav> <div class=\"page_content\"><!-- start content --> <h1 id=\"error-404\">Error 404</h1> <p>Sorry, but our website does not have the page you requested.</p> <p>Maybe you have just a wrong url. Go to <a href=\"/\">www.gutenberg.org</a> to see whether the error persists.</p> <p>If you think something is broken, follow the Contact Information link at the bottom of this page if you would like to report it.</p> </div><!--content ending--> </div><!-- body ending --> <div class=\"footer\"><!-- start footer --> <ul> <li> <a href=\"/policy/privacy_policy.html\" title=\"Privacy Policy\">Privacy policy</a> </li> <li> <a href=\"/about/\" title=\"About Project Gutenberg\">About Project Gutenberg</a> </li> <li> <a href=\"/policy/terms_of_use.html\" title=\"Terms of Use\">Terms of Use</a> </li> <li> <a href=\"/about/contact_information.html\" title=\"Contact Information\">Contact Information</a> </li> <li><a href=\"/help/\" title=\"Get Help\">Get Help</a></li> </ul> <a href=\"https://www.ibiblio.org/\" title=\"Project Gutenberg is hosted by ibiblio\"> <img src=\"/gutenberg/ibiblio-logo.png\" alt=\"iBiblio\"> </a> </div><!-- footer ending--> </body> </html>'\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract KWIC and lexical dispersion plots for keywords of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from re import finditer\n",
    "from itertools import tee, islice, chain, repeat\n",
    "from pprint import pprint \n",
    "from prettytable import PrettyTable\n",
    "from IPython.display import HTML\n",
    "from ipywidgets.widgets import interact\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def kwic(text, tgtword, width=10):\n",
    "    'Find all occurrences of tgtword and show the surrounding context'\n",
    "    matches = (mo.span() for mo in finditer(r\"[A-Za-z\\'\\-]+\", text))\n",
    "    padded = chain(repeat((0,0), width), matches, repeat((-1,-1), width))\n",
    "    t1, t2, t3 = tee((padded), 3)\n",
    "    t2 = islice(t2, width, None)\n",
    "    t3 = islice(t3, 2*width, None)\n",
    "    for (start, _), (i, j), (_, stop) in zip(t1, t2, t3):\n",
    "        if text[i: j] == tgtword:\n",
    "            context = text[start: stop]\n",
    "            yield context, (i,j)\n",
    "            \n",
    "\n",
    "@interact(word='', title=dff.title, size=range(10,25))\n",
    "def get_kwic(title='awakening',word='love', size=25):\n",
    "    \n",
    "    text = str(df[df[0]==title][1].values)\n",
    "    \n",
    "    poss=[]\n",
    "    rows=[]\n",
    "    for t, pos in list(kwic(text, word, size)):       \n",
    "        cols = t.split()\n",
    "\n",
    "        left = ' '.join(cols[0:int(size)])\n",
    "        right = ' '.join(cols[int(size)+1:-1])\n",
    "        rows.append([left, word, right])\n",
    "        poss.append(pos)\n",
    "\n",
    "    x = PrettyTable()\n",
    "    x.field_names = ['left','word','right']\n",
    "    for r in rows:\n",
    "        x.add_row(r)\n",
    "        \n",
    "    if len(rows):\n",
    "        dispersion_plot =  np.zeros(len(text))\n",
    "        for p in poss:\n",
    "            dispersion_plot[p[0]:p[1]] = 1\n",
    "\n",
    "        plt.figure(figsize=(12,5))\n",
    "        \n",
    "        s = pd.Series(dispersion_plot).rolling(5000).sum()\n",
    "        s = pd.DataFrame(s).reset_index()\n",
    "        s['index'] = s['index']/(len(s)/100)\n",
    "        s.set_index('index', inplace=True)\n",
    "        ax=plt.gca()\n",
    "        s.plot(ax=ax)\n",
    "        \n",
    "        table = x.get_html_string().replace(word,'<b>'+word+'</b>')\n",
    "        \n",
    "        return HTML(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Contextual Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(word='', title=dff.title, size=range(11,27,2))\n",
    "def get_mi(title='moby_dick',word='nature', size=27):\n",
    "\n",
    "    text = str(df[df[0]==title][1].values)\n",
    "    cv_all = CountVectorizer(max_features=100000,stop_words='english', lowercase=True)\n",
    "    m_all = cv_all.fit_transform([text]).todense().flatten()\n",
    "\n",
    "    texts = []\n",
    "    for t,pos in list(kwic(text, word, size)):       \n",
    "        cols = t.split(' ')\n",
    "        left = ' '.join(cols[0:int(size)])\n",
    "        right = ' '.join(cols[int(size)+1:-1])\n",
    "        texts.append(left +' ' + right)\n",
    "\n",
    "    if len(texts):\n",
    "\n",
    "        cv = CountVectorizer(max_features=10000, stop_words='english',lowercase=True)\n",
    "        m = cv.fit_transform(texts)\n",
    "\n",
    "        hits = m.shape[0]\n",
    "\n",
    "        names = cv.get_feature_names()\n",
    "        m = m.todense().sum(axis=0)\n",
    "\n",
    "        inds = []\n",
    "        D = {}\n",
    "        for i,item in enumerate(cv_all.get_feature_names()):\n",
    "            D[item] = i\n",
    "\n",
    "\n",
    "        for n in names:\n",
    "            inds.append(D[n])\n",
    "\n",
    "        a = np.asarray(m_all[0].tolist()[0])[np.asarray(inds)]\n",
    "\n",
    "        p_a_b = [_/(hits*size*2) for _ in m]\n",
    "        p_b = hits/len(text)\n",
    "        p_a = [_/len(text) for _ in a]\n",
    "\n",
    "        scores = [(a_b/(a*p_b)) for a_b, a in zip(p_a_b, p_a)]\n",
    "        \n",
    "        print(\"Total number of occurences: {}\".format(hits))\n",
    "        result_list = [(n, s) for n,s in zip(names, scores[0].tolist()[0])]\n",
    "        result = pd.DataFrame(result_list).sort_values(1, ascending=False).head(25)\n",
    "        \n",
    "        return result \n",
    "    \n",
    "    else:\n",
    "        print('please enter term appearing in document')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get top overall words for text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "@interact(title=dff.title)\n",
    "def get_top_terms(title='moby_dick', size=range(15,50,5)):\n",
    "    scores = dff[dff.title==title].drop('title', axis=1).values\n",
    "    inds = scores.argsort()[0][-size:]\n",
    "\n",
    "    result = pd.DataFrame([(a[0],b) for a,b in \n",
    "                           zip(np.asarray(tfidf.get_feature_names()).reshape(-1,1)[inds], \n",
    "                               scores[0][inds])])\n",
    "    result.sort_values(1, ascending=False, inplace=True)\n",
    "    return result \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Get similarity between all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "c = dff.drop('title',axis=1).T.corr()\n",
    "c = c.rename(columns={i:v for i,v in dff.title.items()},index={i:v for i,v in dff.title.items()})\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(c, vmin=-.05, vmax=.05,cmap=\"RdBu_r\")\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 2000\n",
    "n_components = 15\n",
    "n_top_words = 10\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "    \n",
    "\n",
    "def split_str(seq, chunk, skip_tail=False):\n",
    "    lst = []\n",
    "    if chunk <= len(seq):\n",
    "        lst.extend([seq[:chunk]])\n",
    "        lst.extend(split_str(seq[chunk:], chunk, skip_tail))\n",
    "    elif not skip_tail and seq:\n",
    "        lst.extend([seq])\n",
    "    return lst\n",
    "\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', lowercase=True)\n",
    "\n",
    "cv.fit(df[1].values)\n",
    "tf = cv.transform(df[1].values)\n",
    "\n",
    "\n",
    "lda.fit(tf)\n",
    "\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = cv.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
